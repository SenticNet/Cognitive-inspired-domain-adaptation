{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time,re\n",
    "import numpy as np\n",
    "from os import environ, listdir, makedirs\n",
    "from os.path import dirname, exists, expanduser, isdir, join, splitext\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk \n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from senticnet.senticnet import Senticnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def microtext(tweet):\n",
    "    tweet = re.sub(r'(\\$+)([a-zA-Z]+[.]?[a-zA-Z]*)', r'\\2', tweet) #$AAPL -> aapl company\n",
    "    tweet = re.sub(r'&#39;', r\"'\", tweet)\n",
    "    tweet = re.sub(r'&amp;', r'&', tweet)\n",
    "    tweet = re.sub(r'&quot;', r'', tweet)\n",
    "    tweet = re.sub(r'(?:[\\.]{2,10})', ' ', tweet) #delete suspension points\n",
    "    tweet = re.sub(r'(?:[\\!]{2,10})', '!', tweet) #delete repetition of !\n",
    "    tweet = re.sub(r'(?:[\\?]{2,10})', '?', tweet) #delete repetition of ?\n",
    "    tweet = re.sub(r'((?:(?<!\\S)\\.+\\d+-?)+(\\d+)?)', r'0\\1', tweet) #fix decimals .88 -> 0.88\n",
    "    tweet = re.sub(r'[w]{3,10}[\\.]+[\\w]+[\\.]?[a-z]*', '', tweet) #delete websites with no http\n",
    "    tweet = re.sub(r'((?<!\\S))(?:IP)([0-9xX]?)((?!\\S)|[,.\\\\-_])', r'iphone\\2', tweet)\n",
    "    tweet = re.sub(r'((?<!\\S))(?:ip)([0-9xX]?)((?!\\S)|[,.\\\\-_])', r'iphone\\2', tweet)\n",
    "    tweet = re.sub(r'((?<!\\S))(?:Ip)([0-9xX]?)((?!\\S)|[,.\\\\-_])', r'iphone\\2', tweet)\n",
    "    tweet = re.sub(r'((?<!\\S))(?:iP)([0-9xX]?)((?!\\S)|[,.\\\\-_])', r'iphone\\2', tweet)\n",
    "    tweet = re.sub(r'((btfd)|(BTFD)|(Btfd))', 'buy the failed dip', tweet)\n",
    "    tweet = re.sub(r'((btd)|(BTD)|(Btd))', 'buy the dip', tweet) \n",
    "    tweet = re.sub(r'((h8t)|(H8T)|(H8t))', 'hate', tweet)\n",
    "    tweet = re.sub(r'((btmfd)|(BTMFD)|(Btmfd))', 'burn this mother fucker down', tweet)\n",
    "    tweet = re.sub(r'((wtf)|(WTF)|(Wtf))', 'what a fuck', tweet)\n",
    "    tweet = re.sub(r'((lmao)|(LMAO)|(Lmao))', 'laughing my ass off', tweet)\n",
    "    tweet = re.sub(r'((lmfao)|(LMFAO)|(Lmfao))', 'laughing my fucking ass off', tweet)\n",
    "    tweet = re.sub(r'((imho)|(IMHO)|(Imho))', 'in my humble opinion', tweet)\n",
    "    tweet = re.sub(r'((plz)|(PLZ)|(Plz))', 'please', tweet)\n",
    "    tweet = re.sub(r'((govt)|(GOVT)|(Govt))', 'government', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)[n](?!\\S)', 'and', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)([U]|[u])(?!\\S)', 'you', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)[w][\\/]?(?!\\S)', 'with', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)[b](?!\\S)', 'be', tweet) #but b could means also buy\n",
    "    tweet = re.sub(r'(?<!\\S)[t](?!\\S)', 'the', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)(aint)(?!\\S)', 'is not', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)(rez)(?!\\S)', 'reservation', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((sep)|(SEP)|(sept)|(SEPT)|(Sept))(?!\\S)', 'september', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((jan)|(JAN)|(jann)|(JANN)|(Jan))(?!\\S)', 'january', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((feb)|(FEB)|(febr)|(FEBR)|(Feb))(?!\\S)', 'february', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((mar)|(MAR)|(Mar))(?!\\S)', 'march', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((apr)|(APR)|(Apr))(?!\\S)', 'april', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((aug)|(AUG)|(Aug))(?!\\S)', 'august', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((oct)|(OCT)|(Oct))(?!\\S)', 'october', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((nov)|(NOV)|(Nov))(?!\\S)', 'november', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((dec)|(DEC)|(Dec))(?!\\S)', 'december', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((HOD)|(hod)|(Hod))(?!\\S)?', 'head of department', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((BS)|(bs)|(Bs))(?!\\S)', 'bullshit', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((ws)|(WS)|(Ws))(?!\\S)', 'wall street', tweet)\n",
    "    tweet = re.sub(r'((AAPL)|(aapl)|(Aapl))', r'apple', tweet)\n",
    "    tweet = re.sub(r'((QQQ)|(qqq)|(Qqq))', r'NASDAQ 100 index tracking stock', tweet)\n",
    "    tweet = re.sub(r'((CRM)|(crm)|(Crm))', r'salesforce', tweet)\n",
    "    tweet = re.sub(r'((IMNP)|(imnp)|(Imnp))', r'immune pharmaceuticals', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((DRYS)|(drys)|(Drys))(?!\\S)', r'dryships', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((TOPS)|(tops)|(Tops))(?!\\S)', r'top ships', tweet)\n",
    "    tweet = re.sub(r'((DCTH)|(dcth)|(Dtch))', r'delcath systems', tweet)\n",
    "    tweet = re.sub(r'((AVEO)|(aveo)|(Aveo))', r'aveo pharmaceuticals', tweet)\n",
    "    tweet = re.sub(r'((OPXAW)|(opxaw)|(Opxaw))', r'opexa therapeutics', tweet)\n",
    "    tweet = re.sub(r'((GRPN)|(grpn)|(Grpn))', r'groupon', tweet)\n",
    "    tweet = re.sub(r'((MSFT)|(msft)|(Msft))', r'microsoft', tweet)\n",
    "    tweet = re.sub(r'((AMLP)|(amlp)|(Amlp))', r'alerian mlp', tweet)\n",
    "    tweet = re.sub(r'((TSLA)|(tsla)|(Tsla))', r'tesla', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((BABA)|(baba)|(Baba))(?!\\S)', r'alibaba', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((VIX)|(vix)|(Vix))(?!\\S)', r'volatility index', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((SPY)|(spy)|(Spy))(?!\\S)', r'spdr s&p 500', tweet)\n",
    "    tweet = re.sub(r'((NVDA)|(nvda)|(Nvda))', r'nvidia', tweet)\n",
    "    tweet = re.sub(r'((GOOG)|(goog)|(Goog)|(GOOGL)|(googl)|(Googl))(?!\\S)', r'google', tweet)\n",
    "    tweet = re.sub(r'((FB)|(fb)|(Fb))', r'facebook', tweet)\n",
    "    tweet = re.sub(r'((NFLX)|(nflx)|(Nflx))', r'netflix', tweet)\n",
    "    tweet = re.sub(r'(SHOP)(?!\\S)', r'shopify', tweet)\n",
    "    tweet = re.sub(r'((BIIB)|(biib)|(Biib))', r'biogen idec', tweet)\n",
    "    tweet = re.sub(r'((LRCX)|(lrcx)|(Lrcx))', r'lam research', tweet)\n",
    "    tweet = re.sub(r'((AVGO)|(avgo)|(Avgo))', r'avago technologies', tweet)\n",
    "    tweet = re.sub(r'((XLF)|(xlf)|(Xlf))', r'financial select sector spdr', tweet)\n",
    "    tweet = re.sub(r'((TLT)|(tlt)|(Tlt))', r'ishares lehman 20 year', tweet)\n",
    "    tweet = re.sub(r'((IWM)|(iwm)|(Iwm))', r'ishares russel 2000', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((GS)|(gs)|(Gs))(?!\\S)', r'goldman sachs', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((BAC)|(bac))(?!\\S)', r'bank of america', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((CAT)|(cat)|(Cat))(?!\\S)', r'caterpillar', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((FL)|(fl)|(Fl))(?!\\S)', r'foot locker', tweet)\n",
    "    tweet = re.sub(r'((WMT)|(wmt)|(Wmt))', r'walmart', tweet)\n",
    "    tweet = re.sub(r'(?<!\\S)((CRUS)|(crus)|(Crus))(?!\\S)', r'cirrus logic', tweet)\n",
    "    return (tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#L&m on finance\n",
    "with open('/Users/FINANCE_DOMAIN.txt') as nuovo:\n",
    "    result = list(list(l for l in e.split(\"','\") if l) for e in nuovo.read().split(\"\\n\"))\n",
    "result = [x for x in result if not x == []]\n",
    "result = [x for x in result if not \"None'\" in x]\n",
    "#remove duplicate\n",
    "df = pd.DataFrame(result)\n",
    "df = df.drop_duplicates()\n",
    "result = df.values.tolist()\n",
    "#substitute bearish with negative and bullish with positive\n",
    "for i in range(len(result)):\n",
    "    if \"{'basic': 'Bullish'}'\" in result[i][3]:\n",
    "         result[i][3] = \"positive\"\n",
    "    else:\n",
    "        result[i][3] = \"negative\"\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'(?:[\\.]{2,10})', #suspension points\n",
    "    r'(?:[Ss]+[&]+[Pp]+)',\n",
    "    r'\\\\(?:[a-z0-9]{3,3}[\\\\])+[a-z0-9]{3,3}', #unicode\n",
    "    r\"(?:\\$+[\\w_]*[.]?[\\d]*)\", #jargons\n",
    "    #r'\\\\n\\\\n',#enter and next line\n",
    "    r'\\\\n',\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-.]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "    r'(?:(?:\\.?\\d+,?-?%?\\$?)+(?:\\.?\\/?\\d+\\/?\\d*)?(?:[cb])?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def preprocess(s, lowercase=True):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "punctuation = ['\"', '#', '$', '%', '(', ')', '*', '-', ',', '/', '<', '>', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "stop = punctuation + ['\\\\n', 'etc']\n",
    "list_LM = []\n",
    "lemmas=set()\n",
    "POSITIVENESS = []\n",
    "NEGATIVENESS = []\n",
    "with open('/Users/LM/Whole.txt','r') as whole, open('/Users/LM/Positive.txt','r') as pos, open('/Users/LM/Negative.txt','r') as neg:        \n",
    "    for line in whole:\n",
    "        words = [term for term in preprocess(line) if term not in stop]\n",
    "        for w in words:\n",
    "            list_LM.append(w)\n",
    "    for line in pos:\n",
    "        words = [term for term in preprocess(line) if term not in stop] \n",
    "        for w in words:\n",
    "            POSITIVENESS.append(w)\n",
    "    for line in neg:\n",
    "        words = [term for term in preprocess(line) if term not in stop]\n",
    "        for w in words:\n",
    "            NEGATIVENESS.append(w)\n",
    "\n",
    "\n",
    "new_sentiment = {}\n",
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []\n",
    "list_corpus = []\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def penn_to_wn(tag):\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def add_words(lemma):\n",
    "    count_posi = 0\n",
    "    count_nega = 0        \n",
    "    for i in range(len(result)):\n",
    "        ID = result[i][0]\n",
    "        tweet = result[i][2]\n",
    "        label = result[i][3]\n",
    "        tweet = microtext(tweet)\n",
    "        terms_stop = [term for term in preprocess(tweet) if term not in stop and not term.startswith(('http', '\\\\x', '#'))]\n",
    "        tagged_sentence = pos_tag(terms_stop)\n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                continue\n",
    "\n",
    "            lemmat = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemmat:\n",
    "                continue\n",
    "            if lemma == lemmat:\n",
    "                if label == 'positive':\n",
    "                    count_posi += 1\n",
    "                else:\n",
    "                    count_nega += 1\n",
    "            else:\n",
    "                continue\n",
    "    unbalance_ratio = 5\n",
    "    if (count_posi > (count_nega*unbalance_ratio)) and (count_posi > unbalance_ratio):\n",
    "        new_sentiment[lemma] = 0.5\n",
    "        lemmas.add(lemma)\n",
    "    elif ((count_posi*unbalance_ratio) < count_nega) and (count_nega > unbalance_ratio):\n",
    "        new_sentiment[lemma] = -0.5\n",
    "        lemmas.add(lemma)\n",
    "\n",
    "\n",
    "#compute the percentage of right prediction of tweets that have the specific word. The word here has the score not updated\n",
    "def baseline_performance(baseline_word):\n",
    "    right_1 = 0\n",
    "    wrong_1 = 0\n",
    "    for lll in range(len(test_data)):\n",
    "        check_word_in_sentence = []\n",
    "        tagged_sentence = pos_tag(test_data[lll])\n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                continue\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    "            check_word_in_sentence.append(lemma)\n",
    "        if baseline_word in check_word_in_sentence:\n",
    "            compare = test_labels[lll]\n",
    "            baseline_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "            for yyy in check_word_in_sentence:\n",
    "                if yyy in list_new:\n",
    "                    sentiment = 0.0\n",
    "                    if (yyy == baseline_word) and (baseline_word in CHANGE_IN_THE_LEXICON):\n",
    "                        sentiment = CHANGE_IN_THE_LEXICON[baseline_word] \n",
    "                    else:\n",
    "                        if yyy in new_sentiment.keys():\n",
    "                            sentiment = new_sentiment[yyy]\n",
    "                        else:\n",
    "                            if yyy in POSITIVENESS:\n",
    "                                sentiment = 1\n",
    "                            elif yyy in NEGATIVENESS:\n",
    "                                sentiment = -1\n",
    "                    if yyy == 'not':\n",
    "                        sentiment = 0.0\n",
    "                    indexes = word_index[yyy]\n",
    "                    baseline_word_score[indexes] = sentiment\n",
    "                else:\n",
    "                    continue\n",
    "            prova = []\n",
    "            prova.append(baseline_word_score)\n",
    "            prediction_liblinear = classifier_liblinear.predict(prova)\n",
    "            if 'not' in check_word_in_sentence:\n",
    "                if prediction_liblinear == 'positive':\n",
    "                    prediction_liblinear = 'negative'\n",
    "                else:\n",
    "                    prediction_liblinear = 'positive'\n",
    "                    \n",
    "            if compare == prediction_liblinear:\n",
    "                right_1 += 1\n",
    "            else:\n",
    "                wrong_1 += 1    \n",
    "        else:\n",
    "            continue\n",
    "    baseline_performance.performance_baseline = (right_1/(right_1+wrong_1))\n",
    "    return None\n",
    "                \n",
    "def choose_the_best_arm(position):\n",
    "    already_modified = {}\n",
    "    for j in range(len(position)):\n",
    "        baseline_word = list_new[position[j]]\n",
    "        already_modified[baseline_word] = False\n",
    "        if baseline_word in CHANGE_IN_THE_LEXICON.keys():\n",
    "            #narrow the interval and the number of max interations cosi tengo conto (parzialmente anche di tutte gli altri cambiamenti che sto facendo => considere le interdipendenze tra le parole)\n",
    "            already_modified[baseline_word] = True\n",
    "            continue\n",
    "        else:\n",
    "            print(list_new[position[j]], ' try to change its polarity from PS = ', test_word_score[position[j]], 'to: ')\n",
    "            for h in range(0,10):\n",
    "                old_score = test_word_score[position[j]]\n",
    "                new_score = old_score + uniform(-1,1)\n",
    "                performance_baseline = 0.0\n",
    "                baseline_performance(baseline_word)\n",
    "                print(\"PS^ = \", new_score)\n",
    "                right = 0 \n",
    "                wrong = 0\n",
    "                performance = 0.0\n",
    "                for ll in range(len(test_data)):\n",
    "                    check_word_in_sentence = []\n",
    "                    tagged_sentence = pos_tag(test_data[ll])\n",
    "                    for word, tag in tagged_sentence:\n",
    "                        wn_tag = penn_to_wn(tag)\n",
    "                        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                            continue\n",
    "                        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "                        if not lemma:\n",
    "                            continue\n",
    "                        check_word_in_sentence.append(lemma)\n",
    "                    if list_new[position[j]] in check_word_in_sentence:\n",
    "                            #print('ce un altro errore')\n",
    "                        compare = test_labels[ll]\n",
    "                        new_test_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "                        for yy in check_word_in_sentence:\n",
    "                            if yy in list_new:\n",
    "                                sentiment = 0.0\n",
    "                                if yy == list_new[position[j]]:\n",
    "                                    sentiment = new_score\n",
    "                                else:\n",
    "                                    \n",
    "                                    if yy in new_sentiment.keys():\n",
    "                                        sentiment = new_sentiment[yy]\n",
    "                                    else:\n",
    "                                        if yy in POSITIVENESS:\n",
    "                                            sentiment = 1\n",
    "                                        elif yy in NEGATIVENESS:\n",
    "                                            sentiment = -1\n",
    "                                if yy == 'not':\n",
    "                                    sentiment = 0.0\n",
    "                                indexes = word_index[yy]\n",
    "                                new_test_word_score[indexes] = sentiment\n",
    "                            else:\n",
    "                                continue\n",
    "                        prova = []\n",
    "                        prova.append(new_test_word_score)\n",
    "                        prediction_liblinear = classifier_liblinear.predict(prova)\n",
    "                        if 'not' in check_word_in_sentence:\n",
    "                            if prediction_liblinear == 'positive':\n",
    "                                prediction_liblinear = 'negative'\n",
    "                            else:\n",
    "                                prediction_liblinear = 'positive'\n",
    "                        if compare == prediction_liblinear:\n",
    "                            right += 1\n",
    "                        else:\n",
    "                            wrong += 1    \n",
    "                    else:\n",
    "                        continue\n",
    "                performance = (right/(right+wrong))\n",
    "            #print('new perf %f' % (performance))\n",
    "                    #check if there is an improvement\n",
    "                if performance > baseline_performance.performance_baseline:\n",
    "                    print(\"PS' = \", new_score)\n",
    "                    word_with_new_score = list_new[position[j]]\n",
    "                    #if performance < 0.6:\n",
    "                        #riprova[word_with_new_score] = performance\n",
    "                        #print(word_with_new_score, ' aggiunto alla lista dei cattivi')\n",
    "                    ALREADY_MODIFIED[word_with_new_score] = 1\n",
    "                    CHANGE_IN_THE_LEXICON[word_with_new_score] = new_score #cosi cosnidero i cambiamenti solo da un tweet all'altro e non per ogni parola\n",
    "                    riprova_tutto[word_with_new_score] = performance\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "    for g in already_modified:\n",
    "        if already_modified[g] == False:\n",
    "            continue\n",
    "        else:\n",
    "            ALREADY_MODIFIED[g] += 1\n",
    "            if ALREADY_MODIFIED[g] > 9:\n",
    "                continue\n",
    "            else:\n",
    "                print(g, \" try to change its polarity from PS' = \", CHANGE_IN_THE_LEXICON[g], 'to: ')\n",
    "                for w in range(0,10-ALREADY_MODIFIED[g]):\n",
    "                    old_score = CHANGE_IN_THE_LEXICON[g]\n",
    "                    new_score = old_score + uniform(-1/ALREADY_MODIFIED[g],1/ALREADY_MODIFIED[g])\n",
    "                    print(\"PS^^ = \", new_score)\n",
    "                    performance_baseline = 0.0\n",
    "                    baseline_word = g\n",
    "                    baseline_performance(baseline_word)\n",
    "                    right = 0 \n",
    "                    wrong = 0\n",
    "                    performance = 0.0\n",
    "                        #change the score of the word\n",
    "                    for ll in range(len(test_data)):\n",
    "                        check_word_in_sentence = []\n",
    "                        tagged_sentence = pos_tag(test_data[ll])\n",
    "                        for word, tag in tagged_sentence:\n",
    "                            wn_tag = penn_to_wn(tag)\n",
    "                            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                                continue\n",
    "                            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "                            if not lemma:\n",
    "                                continue\n",
    "                            check_word_in_sentence.append(lemma)\n",
    "                        if g in check_word_in_sentence:\n",
    "                            compare = test_labels[ll]\n",
    "                            new_test_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "                            for yy in check_word_in_sentence:\n",
    "                                if yy in list_new:\n",
    "                                    sentiment = 0.0\n",
    "                                    if yy == g:\n",
    "                                        sentiment = new_score\n",
    "                                    else:\n",
    "                                        \n",
    "                                        if yy in new_sentiment.keys():\n",
    "                                            sentiment = new_sentiment[yy]\n",
    "                                        else:\n",
    "                                            if yy in POSITIVENESS:\n",
    "                                                sentiment = 1\n",
    "                                            elif yy in NEGATIVENESS:\n",
    "                                                sentiment = -1\n",
    "                                    if yy == 'not':\n",
    "                                        sentiment = 0.0\n",
    "                                    indexes = word_index[yy]\n",
    "                                    new_test_word_score[indexes] = sentiment\n",
    "                                else:\n",
    "                                    continue\n",
    "                            prova = []\n",
    "                            prova.append(new_test_word_score)\n",
    "                            prediction_liblinear = classifier_liblinear.predict(prova)\n",
    "                            if 'not' in check_word_in_sentence:\n",
    "                                if prediction_liblinear == 'positive':\n",
    "                                    prediction_liblinear = 'negative'\n",
    "                                else:\n",
    "                                    prediction_liblinear = 'positive'\n",
    "                            if compare == prediction_liblinear:\n",
    "                                right += 1\n",
    "                            else:\n",
    "                                wrong += 1    \n",
    "                        else:\n",
    "                            continue\n",
    "                    performance = (right/(right+wrong))\n",
    "            #print('new perf %f' % (performance))\n",
    "                    #check if there is an improvement\n",
    "                    if performance > baseline_performance.performance_baseline:\n",
    "                        word_with_new_score = g\n",
    "                        print(\"PS'' = \", new_score)\n",
    "                        #if performance < 0.6:\n",
    "                            #riprova[word_with_new_score] = performance\n",
    "                            #print(word_with_new_score, ' aggiunto alla lista dei cattivi')\n",
    "                        #if  (old_score > 0.0 and new_score < 0.0) or (old_score < 0.0 and new_score > 0.0):\n",
    "                            #riprova[word_with_new_score] = performance\n",
    "                            #print(word_with_new_score, ' aggiunto alla lista dei cattivi')\n",
    "                        CHANGE_IN_THE_LEXICON[word_with_new_score] = new_score \n",
    "                        #per_frank[(word_with_new_score,old_score)] = new_score\n",
    "                        riprova_tutto[word_with_new_score] = performance\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_just_apple = 0\n",
    "for i in range(len(result)):\n",
    "    print(str(i)+'th record processed')\n",
    "    ID = result[i][0]\n",
    "    tweet = result[i][2]\n",
    "    label = result[i][3]\n",
    "    tweet = microtext(tweet)\n",
    "    terms_stop = [term for term in preprocess(tweet) if term not in stop and not term.startswith(('http', '\\\\x', '#'))]\n",
    "    tagged_sentence = pos_tag(terms_stop)\n",
    "    sentence_lemmas = []\n",
    "    for word, tag in tagged_sentence:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "            continue\n",
    "\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            continue\n",
    "        list_corpus.append(lemma)\n",
    "        sentence_lemmas.append(lemma)\n",
    "        \n",
    "    corpus = set(list_corpus)\n",
    "    set_lemmas = set(sentence_lemmas)\n",
    "    print('lemmatized')\n",
    "    print(set_lemmas.intersection(lemmas))\n",
    "    #add words to sent lex\n",
    "    if (set_lemmas & lemmas) and (set_lemmas.intersection(lemmas) != {'apple'}):\n",
    "        continue\n",
    "    else: \n",
    "        vector_just_apple +=1\n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                continue\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    "            #assign polarity\n",
    "            if (wn_tag in (wn.VERB, wn.ADJ)) and (lemma not in lemmas):\n",
    "                print('prepare to add word')\n",
    "                add_words(lemma)\n",
    "                print('added word')\n",
    "            else:\n",
    "                continue\n",
    "print('vectors with just apple: ', vector_just_apple)\n",
    "print('lexicon extension: ', new_sentiment)\n",
    "lemmas = set(list_LM)\n",
    "new = lemmas.intersection(corpus)\n",
    "list_new = list(new) \n",
    "word_index = {w: idx for idx, w in enumerate(list_new)}  \n",
    "VOCABULARY_SIZE = len(list_new)  \n",
    "print('VOCABULARY_SIZE %f' % (VOCABULARY_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_pos = 0\n",
    "count_neg = 0    \n",
    "real_train = []\n",
    "real_test = []\n",
    "#split test and training data\n",
    "for n in range(len(result)):\n",
    "    ID = result[n][0]\n",
    "    tweet = result[n][2]\n",
    "    label = result[n][3]\n",
    "    tweet = microtext(tweet)\n",
    "    terms_stop = [term for term in preprocess(tweet) if term not in stop and not term.startswith(('http', '\\\\x', '#'))]\n",
    "    if label == 'positive':\n",
    "        count_pos += 1\n",
    "        if count_pos < 1501:\n",
    "            test_data.append(terms_stop)\n",
    "            test_labels.append(label)\n",
    "        else:\n",
    "            train_data.append(terms_stop)\n",
    "            train_labels.append(label)    \n",
    "    else:\n",
    "        count_neg += 1\n",
    "        if count_neg < 1501:\n",
    "            test_data.append(terms_stop)\n",
    "            test_labels.append(label)\n",
    "        else:\n",
    "            train_data.append(terms_stop)\n",
    "            train_labels.append(label)\n",
    "        \n",
    "#create input for classification        \n",
    "for j in range(len(train_data)):\n",
    "    train_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "    tagged_sentence = pos_tag(train_data[j])\n",
    "    for word, tag in tagged_sentence:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "            continue\n",
    "\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            continue\n",
    "\n",
    "        if lemma in list_new: \n",
    "        # Take the first sense, the most common\n",
    "            sentiment = 0.0   \n",
    "            if lemma in new_sentiment.keys():\n",
    "                sentiment = new_sentiment[lemma]\n",
    "            else:\n",
    "                if lemma in POSITIVENESS:\n",
    "                    sentiment = 1\n",
    "                elif lemma in NEGATIVENESS:\n",
    "                    sentiment = -1\n",
    "            if lemma == 'not':\n",
    "                sentiment = 0.0\n",
    "            indexes = word_index[lemma]\n",
    "            train_word_score[indexes] = sentiment\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    real_train.append(train_word_score)\n",
    "    \n",
    "contiamo = 0\n",
    "CHANGE_IN_THE_LEXICON = {}\n",
    "per_frank = {}\n",
    "ALREADY_MODIFIED = {}\n",
    "riprova = {}\n",
    "riprova_tutto = {}\n",
    "for l in range(len(test_data)): #len(test_data)\n",
    "    controlla_not = []\n",
    "    compare = test_labels[l]\n",
    "    test_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "    temp_test_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "    tagged_sentence = pos_tag(test_data[l])\n",
    "    for word, tag in tagged_sentence:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "            continue\n",
    "\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            continue\n",
    "            \n",
    "        controlla_not.append(lemma)\n",
    "\n",
    "        if lemma in list_new: \n",
    "        # Take the first sense, the most common\n",
    "            sentiment = 0.0  \n",
    "            if lemma in new_sentiment.keys():\n",
    "                sentiment = new_sentiment[lemma]                 \n",
    "            else:\n",
    "                if lemma in POSITIVENESS:\n",
    "                    sentiment = 1\n",
    "                elif lemma in NEGATIVENES:\n",
    "                    sentiment = -1\n",
    "            if lemma == 'not': \n",
    "                sentiment = 0.0\n",
    "            indexes = word_index[lemma]\n",
    "            test_word_score[indexes] = sentiment\n",
    "            temp_test_word_score[indexes] = sentiment\n",
    "            #new_test_word_score[indexes] = sentiment\n",
    "        else:\n",
    "            continue\n",
    "    prova = []\n",
    "    prova.append(test_word_score)\n",
    "    list_rewards = []\n",
    "    classifier_liblinear = svm.LinearSVC()\n",
    "    classifier_liblinear.fit(real_train, train_labels)\n",
    "    prediction_liblinear = classifier_liblinear.predict(prova)\n",
    "    if 'not' in controlla_not:\n",
    "        if prediction_liblinear == 'positive':\n",
    "            prediction_liblinear = 'negative'\n",
    "        else:\n",
    "            prediction_liblinear = 'positive'\n",
    "    if compare == prediction_liblinear:\n",
    "        continue\n",
    "    else:\n",
    "        contiamo +=1\n",
    "        position = []\n",
    "        for q in range(len(test_word_score)):\n",
    "            if test_word_score[q] != 0:\n",
    "                position.append(q)\n",
    "            else:\n",
    "                continue\n",
    "        choose_the_best_arm(position)\n",
    "        \n",
    "print('vector explored: ', contiamo)\n",
    "print('after convergence: ', CHANGE_IN_THE_LEXICON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#just convergence\n",
    "final_real_train = []\n",
    "final_real_test = []        \n",
    "#train again SVM with new scores given by the updated sentiment lexicon final_train_word_score\n",
    "for j in range(len(train_data)):\n",
    "    controlla_not = []\n",
    "    final_train_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "    tagged_sentence = pos_tag(train_data[j])\n",
    "    for word, tag in tagged_sentence:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "            continue\n",
    "\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            continue\n",
    "\n",
    "        if lemma in list_new: \n",
    "        # Take the first sense, the most common\n",
    "            sentiment = 0.0   \n",
    "            if lemma in new_sentiment.keys():\n",
    "                sentiment = new_sentiment[lemma]\n",
    "            else:\n",
    "                if lemma in POSITIVENESS:\n",
    "                    sentiment = 1\n",
    "                elif lemma in NEGATIVENESS:\n",
    "                    sentiment = -1\n",
    "            if lemma in CHANGE_IN_THE_LEXICON.keys():\n",
    "                sentiment = CHANGE_IN_THE_LEXICON[lemma]\n",
    "            if lemma == 'not':\n",
    "                sentiment = 0.0\n",
    "            indexes = word_index[lemma]\n",
    "            final_train_word_score[indexes] = sentiment\n",
    "            #temp.append(sentiment)\n",
    "        else:\n",
    "            continue\n",
    "        #if all(p == 0.0 for p in temp):\n",
    "            #print(1)\n",
    "        #else: \n",
    "            #continue\n",
    "        \n",
    "    final_real_train.append(final_train_word_score)\n",
    "\n",
    "for l in range(len(test_data)):      \n",
    "    final_test_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "    #test_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "    tagged_sentence = pos_tag(test_data[l])\n",
    "    for word, tag in tagged_sentence:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "            continue\n",
    "\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            continue\n",
    "\n",
    "        if lemma in list_new: \n",
    "        # Take the first sense, the most common\n",
    "            sentiment = 0.0   \n",
    "            if lemma in new_sentiment.keys():\n",
    "                sentiment = new_sentiment[lemma]\n",
    "            else:\n",
    "                if lemma in POSITIVENESS:\n",
    "                    sentiment = 1\n",
    "                elif lemma in NEGATIVENESS:\n",
    "                    sentiment = -1\n",
    "            if lemma in CHANGE_IN_THE_LEXICON.keys():\n",
    "                sentiment = CHANGE_IN_THE_LEXICON[lemma]\n",
    "            if lemma == 'not':\n",
    "                sentiment = 0.0\n",
    "            indexes = word_index[lemma]\n",
    "            final_test_word_score[indexes] = sentiment             \n",
    "        else:\n",
    "            continue\n",
    "    final_real_test.append(final_test_word_score)\n",
    "    \n",
    "\n",
    "# Perform classification with SVM, kernel=linear\n",
    "new_classifier_liblinear = svm.LinearSVC()\n",
    "t0 = time.time()\n",
    "new_classifier_liblinear.fit(final_real_train, train_labels)\n",
    "t1 = time.time()\n",
    "ok = 0.000\n",
    "no = 0.000\n",
    "for k in range(len(test_data)):\n",
    "    controlla_not = []\n",
    "    tagged_sentence = pos_tag(test_data[k])\n",
    "    for word, tag in tagged_sentence:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "            continue\n",
    "\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            continue\n",
    "        controlla_not.append(lemma)\n",
    "        \n",
    "    prediction = new_classifier_liblinear.predict(final_real_test[k])\n",
    "    if 'not' in controlla_not:\n",
    "        if prediction == 'positive':\n",
    "            prediction = 'negative'\n",
    "        else:\n",
    "            prediction = 'positive'\n",
    "            \n",
    "    if prediction == test_labels[k]:\n",
    "        ok += 1\n",
    "    else:\n",
    "        no += 1\n",
    "accuracy = (ok/no)*100\n",
    "print('accuracy: ', accuracy)\n",
    "\n",
    "#NOW CHECK WITH REINFORCEMENT\n",
    "updated_real_train = []\n",
    "for j in range(len(train_data)):\n",
    "    updated_train_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "    tagged_sentence = pos_tag(train_data[j])\n",
    "    for word, tag in tagged_sentence:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "            continue\n",
    "\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            continue\n",
    "\n",
    "        if lemma in list_new: \n",
    "        # Take the first sense, the most common\n",
    "            sentiment = 0.0   \n",
    "            if lemma in new_sentiment.keys():\n",
    "                sentiment = new_sentiment[lemma]\n",
    "            else:\n",
    "                if lemma in POSITIVENESS:\n",
    "                    sentiment = 1\n",
    "                elif lemma in NEGATIVENESS:\n",
    "                    sentiment = -1\n",
    "            if lemma in CHANGE_IN_THE_LEXICON.keys():\n",
    "                sentiment = CHANGE_IN_THE_LEXICON[lemma]\n",
    "            if lemma == 'not':\n",
    "                sentiment = 0.0\n",
    "            indexes = word_index[lemma]\n",
    "            updated_train_word_score[indexes] = sentiment\n",
    "            #temp.append(sentiment)\n",
    "        else:\n",
    "            continue\n",
    "        #if all(p == 0.0 for p in temp):\n",
    "            #print(1)\n",
    "        #else: \n",
    "            #continue\n",
    "        \n",
    "    updated_real_train.append(updated_train_word_score) \n",
    "\n",
    "updated_classifier_liblinear = svm.LinearSVC()\n",
    "updated_classifier_liblinear.fit(updated_real_train, train_labels)\n",
    "change_in_the_lexicon = {}\n",
    "\n",
    "for marcia in riprova_tutto.keys():\n",
    "        for h in range(0,10):\n",
    "            print('check if ', marcia, 'was changed in the right way:')\n",
    "            new_score = CHANGE_IN_THE_LEXICON[marcia] + uniform(-1,1)\n",
    "            performance_baseline = 0.0\n",
    "            right = 0 \n",
    "            wrong = 0\n",
    "            performance = 0.0\n",
    "                    #change the score of the word\n",
    "            for ll in range(len(test_data)):\n",
    "                check_word_in_sentence = []\n",
    "                tagged_sentence = pos_tag(test_data[ll])\n",
    "                for word, tag in tagged_sentence:\n",
    "                    wn_tag = penn_to_wn(tag)\n",
    "                    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                        continue\n",
    "                    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "                    if not lemma:\n",
    "                        continue\n",
    "                    check_word_in_sentence.append(lemma)\n",
    "                if marcia in check_word_in_sentence:\n",
    "                            #print('ce un altro errore')\n",
    "                    compare = test_labels[ll]\n",
    "                    new_test_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "                    for yy in check_word_in_sentence:\n",
    "                        if yy in list_new:\n",
    "                            sentiment = 0.0\n",
    "                            if yy == marcia:\n",
    "                                sentiment = new_score\n",
    "                            else:\n",
    "                                if yy in new_sentiment.keys():\n",
    "                                    sentiment = new_sentiment[yy]\n",
    "                                else:\n",
    "                                    if yy in POSITIVENESS:\n",
    "                                        sentiment = 1\n",
    "                                    elif yy in NEGATIVENESS:\n",
    "                                        sentiment = -1\n",
    "                                if yy in CHANGE_IN_THE_LEXICON.keys():\n",
    "                                    sentiment = CHANGE_IN_THE_LEXICON[yy]\n",
    "                            if yy == 'not':\n",
    "                                sentiment = 0.0\n",
    "                            indexes = word_index[yy]\n",
    "                            new_test_word_score[indexes] = sentiment\n",
    "                        else:\n",
    "                            continue\n",
    "                    prova = []\n",
    "                    prova.append(new_test_word_score)\n",
    "                    updated_prediction_liblinear = updated_classifier_liblinear.predict(prova)\n",
    "                    if 'not' in check_word_in_sentence:\n",
    "                        if updated_prediction_liblinear == 'positive':\n",
    "                            updated_prediction_liblinear = 'negative'\n",
    "                        else:\n",
    "                            updated_prediction_liblinear = 'positive'\n",
    "                            \n",
    "                    if compare == updated_prediction_liblinear:\n",
    "                        right += 1\n",
    "                    else:\n",
    "                        wrong += 1    \n",
    "                else:\n",
    "                    continue\n",
    "            performance = (right/(right+wrong))\n",
    "            #print('new perf %f' % (performance))\n",
    "                    #check if there is an improvement\n",
    "            if performance > riprova_tutto[marcia]:\n",
    "                word_with_new_score = marcia\n",
    "                print(\"was not ok, so PS''' = \", new_score)\n",
    "                change_in_the_lexicon[word_with_new_score] = new_score #cosi cosnidero i cambiamenti solo da un tweet all'altro e non per ogni parola\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "CHANGE_IN_THE_LEXICON.update(change_in_the_lexicon)\n",
    "print('after reinforcement and convergence: ', CHANGE_IN_THE_LEXICON)\n",
    "\n",
    "end_real_train = []\n",
    "end_real_test = []        \n",
    "#train again SVM with new scores given by the updated sentiment lexicon final_train_word_score\n",
    "for j in range(len(train_data)):\n",
    "    end_train_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "    tagged_sentence = pos_tag(train_data[j])\n",
    "    for word, tag in tagged_sentence:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "            continue\n",
    "\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            continue\n",
    "\n",
    "        if lemma in list_new: \n",
    "        # Take the first sense, the most common\n",
    "            sentiment = 0.0   \n",
    "            if lemma in new_sentiment.keys():\n",
    "                sentiment = new_sentiment[lemma]\n",
    "            else:\n",
    "                if lemma in POSITIVENESS:\n",
    "                    sentiment = 1\n",
    "                elif lemma in NEGATIVENESS:\n",
    "                    sentiment = -1\n",
    "            if lemma in CHANGE_IN_THE_LEXICON.keys():\n",
    "                sentiment = CHANGE_IN_THE_LEXICON[lemma]\n",
    "            if lemma == 'not':\n",
    "                sentiment = 0.0\n",
    "            indexes = word_index[lemma]\n",
    "            end_train_word_score[indexes] = sentiment\n",
    "            #temp.append(sentiment)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        \n",
    "    end_real_train.append(end_train_word_score)\n",
    "\n",
    "for l in range(len(test_data)):      \n",
    "    end_test_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "    #test_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "    tagged_sentence = pos_tag(test_data[l])\n",
    "    for word, tag in tagged_sentence:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "            continue\n",
    "\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            continue\n",
    "\n",
    "        if lemma in list_new: \n",
    "        # Take the first sense, the most common\n",
    "            sentiment = 0.0   \n",
    "            if lemma in new_sentiment.keys():\n",
    "                sentiment = new_sentiment[lemma]\n",
    "            else:\n",
    "                if lemma in POSITIVENESS:\n",
    "                    sentiment = 1\n",
    "                elif lemma in NEGATIVENESS:\n",
    "                    sentiment = -1\n",
    "            if lemma in CHANGE_IN_THE_LEXICON.keys():\n",
    "                sentiment = CHANGE_IN_THE_LEXICON[lemma]\n",
    "            if lemma == 'not':\n",
    "                sentiment = 0.0\n",
    "            indexes = word_index[lemma]\n",
    "            end_test_word_score[indexes] = sentiment             \n",
    "        else:\n",
    "            continue\n",
    "    end_real_test.append(end_test_word_score)\n",
    "\n",
    "\n",
    "# Perform classification with SVM, kernel=linear\n",
    "end_classifier_liblinear = svm.LinearSVC()\n",
    "end_classifier_liblinear.fit(end_real_train, train_labels)\n",
    "ok = 0.000\n",
    "no = 0.000\n",
    "for k in range(len(test_data)):\n",
    "    controlla_not = []\n",
    "    tagged_sentence = pos_tag(test_data[k])\n",
    "    for word, tag in tagged_sentence:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "            continue\n",
    "\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            continue\n",
    "        controlla_not.append(lemma)\n",
    "        \n",
    "    prediction = end_classifier_liblinear.predict(end_real_test[k])\n",
    "    if 'not' in controlla_not:\n",
    "        if prediction == 'positive':\n",
    "            prediction = 'negative'\n",
    "        else:\n",
    "            prediction = 'positive'\n",
    "            \n",
    "    if prediction == test_labels[k]:\n",
    "        ok += 1.0\n",
    "    else:\n",
    "        no += 1.0\n",
    "accuracy = (ok/no)*100\n",
    "print('accuracy_with_reinforcement: ', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
