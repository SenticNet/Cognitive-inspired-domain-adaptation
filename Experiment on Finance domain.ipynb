{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from os import environ, listdir, makedirs\n",
    "from os.path import dirname, exists, expanduser, isdir, join, splitext\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk \n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from senticnet.senticnet import Senticnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from random import uniform\n",
    "\n",
    "sn = Senticnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULARY_SIZE 3981.000000\n",
      "quante sbaglaite cerano 500\n",
      "{'kid': -0.74119883051033664, 'signal': -0.71086318306468343, 'week': 0.48406910558510818, 'power': 0.30960153109099808, 'leave': 0.78638790724139651, 'phone': 1.0814030461171646, 'use': -0.46118716248958841, 'pixel': 0.77794323243479502, 'down': -0.74584700847119767, 'trade': -0.20462027410042133, 'amazon': -0.31709513091556751, 'sit': -0.88200951145746953, 'wed': -0.08725524967624787, 'break': 1.1326904044950026, 'close': -0.24697661405293775, 'sell': 0.92887726375241086, 'time': 0.33496908100668749, 'least': -1.0879622537879472, 'change': -0.4468375794248981, 'pro': 1.0748711912969571, 'battery': 0.45108123892942165, 'cheap': -0.67781095464126995, 'ipod': -0.26988408029350758, 'take': -0.41820263423651549, 'hole': 1.7227888616800606, 'death': 0.28067158883927112, 'revenue': -0.74915048064869549, 'less': -1.6901861374806353, 'make': -0.33712784509531912, 'weekend': -0.38818454354514198, 'buddy': 1.0296066073358305, 'dump': -0.50614749690078409, 'company': 1.1182122996088788, 'plant': -0.037541492902136708, 'lose': -1.6095925679472489, 'disappointing': -1.5099297116197479, 'very': 0.01989637012727119, 'predict': 0.84451080324457195, 'come': 0.13980393492761339, 'correction': 1.3562193300647372, 'order': 0.90877364055169674, 'fire': 0.79364156247369966, 'mention': 1.1134761239778106, 'steve': -1.6222724417185974, 'remove': -1.0513827042045334, 'lure': 0.85325917531524342, 'grave': 0.1941749084825054, 'retard': -1.2538939553136452, 'china': 0.71928677467996882, 'watch': -0.17759477155937031, 'help': -0.15029592919226453, 'steal': -0.042815924029751318, 'swing': -0.1559002583756115, 'admit': 1.0776277823156462, 'america': 0.53817293153474044, 'price': 0.35172486542923742, 'keep': 0.38311093352041403, 'low': -0.53374949578254904, 'insane': -0.026276799206592871, 'department': 0.71295708336438524, 'short': -1.7774907341905124, 'opinion': -0.2778189983144933, 'tumble': 0.35493618624637746, 'scare': 0.66061677322114964, 'bear': 1.7114171983646762, 'beware': -1.2045450422280903, 'rain': 0.23236253420150454, 'today': 0.095621242820170427, 'same': 1.0019522468485549, 'volatility': -1.7009062106623163, 'open': 1.010787190219768, 'creation': 1.0323370483820429, 'jump': -0.65024561100765943, 'trend': -0.68134340428714235, 'financial': 1.0403937490868116, 'rid': 1.1651006890439874, 'reverse': -0.69979906526166569, 'stream': 0.10820645355140013, 'hurt': -1.1158257568170975, 'growth': -0.19854362996446459, 'load': -0.20223532110569681, 'yes': 0.052465417134071046, 'only': -1.7671115517789056, 'straight': 0.9538043467013112, 'high': 0.36940470338424042, 'event': -0.14280394169777549, 'mobile': 0.5796688842935025, 'wedge': 1.4788049241375663, 'useless': 0.17745922354352506, 'glass': 0.33897496668488247, 'option': 0.10485728996674296, 'side': 0.95433570311335936, 'margin': -0.29116845945972347, 'information': 0.096021665251280641, 'buying': -1.4995724982218177, 'continuation': -0.87657824354231395, 'failure': -0.91780929433962433, 'apparently': 1.2608951021828214, 'drag': 0.59298625631754853, 'tired': 1.3887386283161072, 'poor': -1.1863561429021483, 'hold': 2.1983385162156326, 'grow': 0.92225763693472518, 'delay': -1.5475507915038471, 'live': -0.19083630805868901, 'bluff': -1.7164877163576069, 'war': -0.69326244084476707, 'sad': -1.0492629915031195, 'lag': 0.015048333581478057, 'wearable': -0.97632017853394559, 'night': -1.3148603312387666, 'month': 0.54383965701789561, 'fund': -0.050775425432883714, 'question': 1.1729004467173063, 'volume': 0.28727689919053445, 'end': 0.92769416469045485, 'plan': -0.64568282966302326, 'yeah': -1.7192952543976636, 'little': 0.39888820971850336, 'strong': 0.42498531494237712, 'lower': -0.84275849026072458, 'cent': -0.17210426020551495, 'extreme': 0.71325264211318129, 'immune': -1.1867815793990428, 'year': 0.70096058917565829, 'thought': -0.31813486529896151, 'production': 1.0098974499558704, 'couple': 1.5889141744595463, 'fast': 0.89220360469400184, 'track': -0.30519618499332218, 'ship': -0.87578941606738547, 'back': 1.7734165648458611, 'do': 1.4744538203384954, 'king': -0.8827951775529882, 'act': -0.1510635614933562, 'post': -0.44769776573730213, 'downgrade': -0.91533143343922352, 'have': 0.041627402721171047, 'analyst': -0.32331148121841113, 'worth': 0.82425022269730686, 'mean': 1.5084139245187269, 'put': 1.2240727261179734, 'see': -0.27189122164021817, 'market': -0.28152114451532601, 'country': 0.0084151624814702286, 'begin': 1.0372816057390453, 'innovation': 1.4002928344925469, 'float': -1.5343600950363312, 'guy': 1.1252889385788021, 'weekly': -0.76858729534674097, 'lawsuit': -1.1506614316506731, 'foot': 0.84170009745580743, 'cause': 1.2849354459390967, 'over': 1.0551503183961333, 'apple': 0.65534898072091097, 'big': -0.027458513080924479, 'gold': 1.7039567638764392, 'rally': 1.4702935283681924, 'test': 1.3848760854452835, 'block': 1.2814170605895616, 'breakout': 0.16905086488486909, 'smell': 1.3141879040255697, 'real': 0.74294338612094768, 'last': 0.20667203370842516, 'bring': -0.32623766859523262, 'news': -0.35129253547814554, 'purchase': 0.90273447088025283, 'get': 1.6647012949750921, 'super': -0.45437807299513078, 'push': 1.2131241794600722, 'disaster': -0.69023329439144132, 'pull': -0.076917879499524888, 'head': 0.28374964366429212, 'mark': -1.5712478963895311, 'aware': 0.91842102423921013, 'fly': 0.72532616575408349, 'snap': -0.23151729389804632, 'increase': 0.66695884939497019, 'plummet': -0.95333758303177984, 'technology': -0.55755088237986339, 'medium': 0.76684362995816135, 'bet': 0.29052517523317167, 'link': -1.0791043244946057, 'just': -0.4875408392240731, 'fall': -1.0760875185088574, 'smoke': -0.023271457277483587, 'trump': -0.16704215686122148, 'new': -0.32743060733780149, 'massively': 1.3282682205542466, 'follow': 0.84917176128118932, 'soon': 0.51844348050498779, 'bubble': -0.015002150299524564, 'love': -0.29583039313458515, 'computer': -0.88768979363354938, 'build': 1.8608840124604868, 'chart': 0.62686816013235691, 'expect': 0.31507475142360386, 'leader': -0.46680013735933384, 'humble': 0.019063589728640307, 'fool': -0.13998145848413723, 'deliberately': -1.1294290411282786, 'dis': -1.6851864803320094, 'release': -0.18874674659563129, 'movie': 1.0249917597263569, 'job': -0.89820343772987843, 'note': -0.9462096364443543, 'tim': -1.2795890517029695, 'loss': -0.22077713507549401, 'breach': -1.4213994203380289, 'many': 0.21803668036750612, 'cover': 0.75427336251705934, 'tell': 0.81751881416878547, 'team': 0.78067793247888218, 'learn': 0.79512045718039159, 'maybe': -1.6525748437144283, 'product': 0.60486966325407254, 'index': -0.011245892247135569, 'copy': 0.6362432810977815, 'temporary': -0.59958035607299287, 'liberal': 0.44101605289832191, 'string': -0.71774015975353578, 'remember': 0.16296251207034373, 'clear': 1.3038697639211521, 'cook': 0.79576163877737272, 'level': 0.98805754301368931, 'launch': 0.84474698610601751, 'red': 0.53525790429096243, 'thankful': -0.13632555790388956, 'sign': 0.99123890018181071, 'correct': 0.83700828831603413, 'prepare': -0.34469352704869538, 'september': -0.37185404029486657, 'crash': -0.92886804322698513, 'drop': 0.47939309420624321, 'pain': -1.5615786571729142, 'stock': 0.41391132217592436, 'worthless': -0.018455703354512631, 'guidance': -0.15877024790658101, 'content': -0.50471904635043807, 'deal': 0.87271679543214709, 'give': -0.19872754598955478, 'adjust': 0.77586694878193796, 'embarrass': -0.44204277660993474, 'nervous': 0.40453030840132459, 'try': 0.54564327682454394, 'unable': 0.019704356017498426, 'humanity': -0.4465397283822663, 'example': 1.1811989494079622, 'resolve': 0.20863128494885652, 'hour': -0.25325967526628046, 'pretty': 1.8480974975654596, 'share': 1.5068992536172345, 'first': -0.62776419853018439, 'dip': 0.43769408417545197, 'current': -0.86213278732816989, 'start': 0.22912062952025747, 'add': 0.040057095546771726, 'say': 1.5199157979787619, 'sentiment': -0.43296465451462474, 'position': 0.77186463099651936, 'rise': -0.78469384159005073, 'expensive': -1.5199384807641718, 'know': 1.0628652854550302, 'likely': -0.051166793978059594, 'top': 0.25362674988517503, 'wait': 0.74354421562606787, 'few': -1.33838443411363, 'bit': -0.14665023717478598, 'reversal': 0.13873299840736619, 'charge': 0.89051249915089037, 'superior': -0.33547614428349848, 'fashionable': 0.20632891065725611, 'run': 0.11948936104013308}\n",
      "Results for SVC(kernel=rbf)\n",
      "Training time: 221.930605s; Prediction time: 36.070910s\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.00      0.00      0.00      1000\n",
      "   positive       0.50      1.00      0.67      1000\n",
      "\n",
      "avg / total       0.25      0.50      0.33      2000\n",
      "\n",
      "Accuracy: 0.500000\n",
      "\n",
      "\n",
      "Results for SVC(kernel=linear)\n",
      "Training time: 224.045461s; Prediction time: 35.995465s\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.92      0.15      0.26      1000\n",
      "   positive       0.54      0.99      0.70      1000\n",
      "\n",
      "avg / total       0.73      0.57      0.48      2000\n",
      "\n",
      "Accuracy: 0.569000\n",
      "\n",
      "\n",
      "Results for LinearSVC()\n",
      "Training time: 0.448003s; Prediction time: 0.050289s\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.90      0.24      0.39      1000\n",
      "   positive       0.56      0.97      0.71      1000\n",
      "\n",
      "avg / total       0.73      0.61      0.55      2000\n",
      "\n",
      "Accuracy: 0.609500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zxing001\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    with open('C:\\\\PATH\\\\SAMPLE.txt','r') as old, open('C:\\\\PATH\\\\DATASET.txt','w') as new:\n",
    "        for line in old:\n",
    "            line = line.replace('\"', \"'\")\n",
    "            new.write(line)\n",
    "    with open('C:\\\\PATH\\\\DATASET.txt') as nuovo:\n",
    "        result = list(list(l for l in e.split(\"','\") if l) for e in nuovo.read().split(\"\\n\")) \n",
    "    result = [x for x in result if not x == []]\n",
    "    result = [x for x in result if not \"None'\" in x]\n",
    "    #remove duplicate\n",
    "    df = pd.DataFrame(result)\n",
    "    df = df.drop_duplicates()\n",
    "    result = df.values.tolist()\n",
    "    for i in range(len(result)):\n",
    "        if \"{'basic': 'Bullish'}'\" in result[i][3]:\n",
    "             result[i][3] = \"positive\"\n",
    "        else:\n",
    "            result[i][3] = \"negative\"\n",
    "    #using bonzanini as base  \n",
    "    import re\n",
    "    emoticons_str = r\"\"\"\n",
    "        (?:\n",
    "            [:=;] # Eyes\n",
    "            [oO\\-]? # Nose (optional)\n",
    "            [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "        )\"\"\"\n",
    "    regex_str = [\n",
    "        emoticons_str,\n",
    "        r'(?:[\\.]{2,10})', \n",
    "        r'(?:[Ss]+[&]+[Pp]+)',\n",
    "        r'\\\\(?:[a-z0-9]{3,3}[\\\\])+[a-z0-9]{3,3}', \n",
    "        r\"(?:\\$+[\\w_]*[.]?[\\d]*)\", \n",
    "        r'\\\\n\\\\n',\n",
    "        r'\\\\n',\n",
    "        r'<[^>]+>', # HTML tags\n",
    "        r'(?:@[\\w_]+)', # @-mentions\n",
    "        r\"(?:\\#+[\\w_]+[\\w\\'_\\-.]*[\\w_]+)\", # hash-tags\n",
    "        r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "        r'(?:(?:\\.?\\d+,?-?%?\\$?)+(?:\\.?\\/?\\d+\\/?\\d*)?(?:[cb])?)', # numbers\n",
    "        r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "        r'(?:[\\w_]+)', # other words\n",
    "        r'(?:\\S)' # anything else\n",
    "    ]\n",
    "\n",
    "    tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "    emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "    def tokenize(s):\n",
    "        return tokens_re.findall(s)\n",
    " \n",
    "    def preprocess(s, lowercase=True):\n",
    "        tokens = tokenize(s)\n",
    "        if lowercase:\n",
    "            tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "        return tokens\n",
    "    punctuation = ['\"', '#', '$', '%', '(', ')', '*', '-', ',', '/', '<', '>', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "    stop = punctuation + ['\\\\n', 'etc']\n",
    "    \n",
    "    new_sentiment = {}\n",
    "    lista = list(sn.data.keys())\n",
    "    lemmas = set(lista)    \n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "    list_corpus = []\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def penn_to_wn(tag):\n",
    "        \n",
    "        if tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        elif tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        return None\n",
    "\n",
    "    def add_words(lemma):\n",
    "        count_posi = 0\n",
    "        count_nega = 0        \n",
    "        for i in range(len(result)):\n",
    "            ID = result[i][0]\n",
    "            tweet = result[i][2]\n",
    "            label = result[i][3]\n",
    "            microtext(tweet)\n",
    "            terms_stop = [term for term in preprocess(tweet) if term not in stop and not term.startswith(('http', '\\\\x', '#'))]\n",
    "            tagged_sentence = pos_tag(terms_stop)\n",
    "            for word, tag in tagged_sentence:\n",
    "                wn_tag = penn_to_wn(tag)\n",
    "                if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                    continue\n",
    " \n",
    "                lemmat = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "                if not lemmat:\n",
    "                    continue\n",
    "                if lemma == lemmat:\n",
    "                    if label == 'positive':\n",
    "                        count_posi += 1\n",
    "                    else:\n",
    "                        count_nega += 1\n",
    "                else:\n",
    "                    continue\n",
    "        unbalance_ratio = 5\n",
    "        if count_posi > (count_nega*unbalance_ratio) and count_posi > unbalance_ratio:\n",
    "            new_sentiment[lemma] = 0.5\n",
    "            lemmas.add(lemma)\n",
    "        elif (count_posi*unbalance_ratio) < count_nega and count_nega > unbalance_ratio:\n",
    "            new_sentiment[lemma] = -0.5\n",
    "            lemmas.add(lemma)        \n",
    "        return None\n",
    "    \n",
    "    #compute the percentage of right prediction of tweets that have the specific word. The word here has the score not updated\n",
    "    def baseline_performance(baseline_word):\n",
    "        right_1 = 0\n",
    "        wrong_1 = 0\n",
    "        for lll in range(len(test_data)):\n",
    "            check_word_in_sentence = []\n",
    "            tagged_sentence = pos_tag(test_data[lll])\n",
    "            for word, tag in tagged_sentence:\n",
    "                wn_tag = penn_to_wn(tag)\n",
    "                if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                    continue\n",
    "                lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "                if not lemma:\n",
    "                    continue\n",
    "                check_word_in_sentence.append(lemma)\n",
    "            if baseline_word in check_word_in_sentence:\n",
    "                compare = test_labels[lll]\n",
    "                baseline_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "                for yyy in check_word_in_sentence:\n",
    "                    if yyy in list_new:\n",
    "                        sentiment = 0.0\n",
    "                        sentiment = sn.polarity_intense(yyy)\n",
    "                        if yyy in new_sentiment.keys():\n",
    "                            sentiment += new_sentiment[yyy]\n",
    "                        if yyy in CHANGE_IN_THE_LEXICON.keys():\n",
    "                            sentiment = CHANGE_IN_THE_LEXICON[yyy]\n",
    "                        indexes = word_index[yyy]\n",
    "                        baseline_word_score[indexes] = sentiment\n",
    "                    else:\n",
    "                        continue\n",
    "                prova = []\n",
    "                prova.append(baseline_word_score)\n",
    "                prediction_liblinear = classifier_liblinear.predict(prova)\n",
    "                if compare == prediction_liblinear:\n",
    "                    right_1 += 1\n",
    "                else:\n",
    "                    wrong_1 += 1    \n",
    "            else:\n",
    "                continue\n",
    "        baseline_performance.performance_baseline = (right_1/(right_1+wrong_1))\n",
    "        print('baseline perf: %f' %(baseline_performance.performance_baseline))        \n",
    "        return None\n",
    "                    \n",
    "    def choose_the_best_arm(position):\n",
    "        already_modified = {}\n",
    "        for j in range(len(position)):\n",
    "            baseline_word = list_new[position[j]]\n",
    "            already_modified[baseline_word] = False\n",
    "            if baseline_word in CHANGE_IN_THE_LEXICON.keys():\n",
    "                #narrow the interval and the number of max interations cosi tengo conto (parzialmente anche di tutte gli altri cambiamenti che sto facendo => considere le interdipendenze tra le parole)\n",
    "                already_modified[baseline_word] = True\n",
    "                continue\n",
    "            else:\n",
    "                for h in range(0,11):\n",
    "                    old_score = test_word_score[position[j]]\n",
    "                    new_score = old_score + uniform(-1,1)\n",
    "                    performance_baseline = 0.0\n",
    "                    baseline_performance(baseline_word)\n",
    "                    right = 0 \n",
    "                    wrong = 0\n",
    "                    performance = 0.0\n",
    "                        #change the polarity score of the word\n",
    "                    for ll in range(len(test_data)):\n",
    "                        check_word_in_sentence = []\n",
    "                        tagged_sentence = pos_tag(test_data[ll])\n",
    "                        for word, tag in tagged_sentence:\n",
    "                            wn_tag = penn_to_wn(tag)\n",
    "                            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                                continue\n",
    "                            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "                            if not lemma:\n",
    "                                continue\n",
    "                            check_word_in_sentence.append(lemma)\n",
    "                        if list_new[position[j]] in check_word_in_sentence:\n",
    "                                #print('ce un altro errore')\n",
    "                            compare = test_labels[ll]\n",
    "                            new_test_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "                            for yy in check_word_in_sentence:\n",
    "                                if yy in list_new:\n",
    "                                    sentiment = 0.0\n",
    "                                    if yy == list_new[position[j]]:\n",
    "                                        sentiment = new_score\n",
    "                                    else:\n",
    "                                        sentiment = sn.polarity_intense(yy)\n",
    "                                        if yy in new_sentiment.keys():\n",
    "                                            sentiment += new_sentiment[yy]\n",
    "                                        if yy in CHANGE_IN_THE_LEXICON.keys():\n",
    "                                            sentiment = CHANGE_IN_THE_LEXICON[yy]\n",
    "                                    indexes = word_index[yy]\n",
    "                                    new_test_word_score[indexes] = sentiment\n",
    "                                else:\n",
    "                                    continue\n",
    "                            prova = []\n",
    "                            prova.append(new_test_word_score)\n",
    "                            prediction_liblinear = classifier_liblinear.predict(prova)\n",
    "                            if compare == prediction_liblinear:\n",
    "                                right += 1\n",
    "                            else:\n",
    "                                wrong += 1    \n",
    "                        else:\n",
    "                            continue\n",
    "                    performance = (right/(right+wrong))\n",
    "                    if performance > baseline_performance.performance_baseline:\n",
    "                        word_with_new_score = list_new[position[j]]\n",
    "                        ALREADY_MODIFIED[word_with_new_score] = 1\n",
    "                        change_in_the_lexicon[word_with_new_score] = new_score #cosi cosnidero i cambiamenti solo da un tweet all'altro e non per ogni parola\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "        for g in already_modified:\n",
    "            if already_modified[g] == False:\n",
    "                continue\n",
    "            else:\n",
    "                ALREADY_MODIFIED[g] += 1\n",
    "                if ALREADY_MODIFIED[g] > 10:\n",
    "                    continue\n",
    "                else:\n",
    "                    for w in range(0,11-ALREADY_MODIFIED[g]):\n",
    "                        old_score = CHANGE_IN_THE_LEXICON[g]\n",
    "                        new_score = old_score + uniform(-1/ALREADY_MODIFIED[g],1/ALREADY_MODIFIED[g])\n",
    "                        performance_baseline = 0.0\n",
    "                        baseline_word = g\n",
    "                        baseline_performance(baseline_word)\n",
    "                        right = 0 \n",
    "                        wrong = 0\n",
    "                        performance = 0.0\n",
    "                            #change the score of the word\n",
    "                        for ll in range(len(test_data)):\n",
    "                            check_word_in_sentence = []\n",
    "                            tagged_sentence = pos_tag(test_data[ll])\n",
    "                            for word, tag in tagged_sentence:\n",
    "                                wn_tag = penn_to_wn(tag)\n",
    "                                if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                                    continue\n",
    "                                lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "                                if not lemma:\n",
    "                                    continue\n",
    "                                check_word_in_sentence.append(lemma)\n",
    "                            if g in check_word_in_sentence:\n",
    "                                #print('ce un altro errore')\n",
    "                                compare = test_labels[ll]\n",
    "                                new_test_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "                                for yy in check_word_in_sentence:\n",
    "                                    if yy in list_new:\n",
    "                                        sentiment = 0.0\n",
    "                                        if yy == g:\n",
    "                                            sentiment = new_score\n",
    "                                        else:\n",
    "                                            sentiment = sn.polarity_intense(yy)\n",
    "                                            if yy in new_sentiment.keys():\n",
    "                                                sentiment += new_sentiment[yy]\n",
    "                                            if yy in CHANGE_IN_THE_LEXICON.keys():\n",
    "                                                sentiment = CHANGE_IN_THE_LEXICON[yy]\n",
    "                                        indexes = word_index[yy]\n",
    "                                        new_test_word_score[indexes] = sentiment\n",
    "                                    else:\n",
    "                                        continue\n",
    "                                prova = []\n",
    "                                prova.append(new_test_word_score)\n",
    "                                prediction_liblinear = classifier_liblinear.predict(prova)\n",
    "                                if compare == prediction_liblinear:\n",
    "                                    right += 1\n",
    "                                else:\n",
    "                                    wrong += 1    \n",
    "                            else:\n",
    "                                continue\n",
    "                        performance = (right/(right+wrong))\n",
    "                        if performance > baseline_performance.performance_baseline:\n",
    "                            word_with_new_score = g\n",
    "                            change_in_the_lexicon[word_with_new_score] = new_score #cosi cosnidero i cambiamenti solo da un tweet all'altro e non per ogni parola\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "        return None\n",
    "    \n",
    "    for i in range(len(result)):\n",
    "        ID = result[i][0]\n",
    "        microtext(tweet)\n",
    "        terms_stop = [term for term in preprocess(tweet) if term not in stop and not term.startswith(('http', '\\\\x', '#'))]\n",
    "        tagged_sentence = pos_tag(terms_stop)\n",
    "        sentence_lemmas = []\n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                continue \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    "            list_corpus.append(lemma)\n",
    "            sentence_lemmas.append(lemma)\n",
    "            \n",
    "        corpus = set(list_corpus)\n",
    "        set_lemmas = set(sentence_lemmas)\n",
    "        #add words to sent lex\n",
    "        if set_lemmas & lemmas:\n",
    "            continue\n",
    "        else: \n",
    "            for word, tag in tagged_sentence:\n",
    "                wn_tag = penn_to_wn(tag)\n",
    "                if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                    continue\n",
    " \n",
    "                lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "                if not lemma:\n",
    "                    continue\n",
    "                if wn_tag in (wn.VERB, wn.ADJ) and lemma not in lemmas:\n",
    "                    add_words(lemma) \n",
    "                    print(lemma)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "    new = lemmas.intersection(corpus)\n",
    "    list_new = list(new) \n",
    "    word_index = {w: idx for idx, w in enumerate(list_new)}  \n",
    "    VOCABULARY_SIZE = len(list_new)  \n",
    "    print('VOCABULARY_SIZE %f' % (VOCABULARY_SIZE))\n",
    "    \n",
    "    count_pos = 0\n",
    "    count_neg = 0    \n",
    "    real_train = []\n",
    "    real_test = []\n",
    "    #split test and training data\n",
    "    for n in range(len(result)):\n",
    "        ID = result[n][0]\n",
    "        tweet = result[n][2]\n",
    "        label = result[n][3]\n",
    "        microtext(tweet)\n",
    "        terms_stop = [term for term in preprocess(tweet) if term not in stop and not term.startswith(('http', '\\\\x', '#'))]\n",
    "        if label == 'positive':\n",
    "            count_pos += 1\n",
    "            if count_pos < 1001:\n",
    "                test_data.append(terms_stop)\n",
    "                test_labels.append(label)\n",
    "            else:\n",
    "                train_data.append(terms_stop)\n",
    "                train_labels.append(label)    \n",
    "        else:\n",
    "            count_neg += 1\n",
    "            if count_neg < 1001:\n",
    "                test_data.append(terms_stop)\n",
    "                test_labels.append(label)\n",
    "            else:\n",
    "                train_data.append(terms_stop)\n",
    "                train_labels.append(label)\n",
    "                 \n",
    "    for j in range(len(train_data)):\n",
    "        train_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "        tagged_sentence = pos_tag(train_data[j])\n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                continue\n",
    " \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    "\n",
    "            if lemma in list_new: \n",
    "            # Take the first sense, the most common\n",
    "                sentiment = 0.0   \n",
    "                sentiment = sn.polarity_intense(lemma)\n",
    "                if lemma in new_sentiment.keys():\n",
    "                    sentiment += new_sentiment[lemma]\n",
    "                else:\n",
    "                    pass\n",
    "                indexes = word_index[lemma]\n",
    "                train_word_score[indexes] = sentiment\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        real_train.append(train_word_score)\n",
    "        \n",
    "    contiamo = 0\n",
    "    CHANGE_IN_THE_LEXICON = {}\n",
    "    ALREADY_MODIFIED = {}\n",
    "    for l in range(len(test_data)): \n",
    "        compare = test_labels[l]\n",
    "        test_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "        temp_test_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "        tagged_sentence = pos_tag(test_data[l])\n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                continue\n",
    " \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    " \n",
    "            if lemma in list_new: \n",
    "            # Take the first sense, the most common\n",
    "                sentiment = 0.0   \n",
    "                sentiment = sn.polarity_intense(lemma)\n",
    "                if lemma in new_sentiment.keys():\n",
    "                    sentiment += new_sentiment[lemma] \n",
    "                if lemma in CHANGE_IN_THE_LEXICON.keys():\n",
    "                    sentiment = CHANGE_IN_THE_LEXICON[lemma] \n",
    "                indexes = word_index[lemma]\n",
    "                test_word_score[indexes] = sentiment\n",
    "                temp_test_word_score[indexes] = sentiment\n",
    "            else:\n",
    "                continue\n",
    "        prova = []\n",
    "        prova.append(test_word_score)\n",
    "        list_rewards = []\n",
    "        classifier_liblinear = svm.LinearSVC()\n",
    "        classifier_liblinear.fit(real_train, train_labels)\n",
    "        prediction_liblinear = classifier_liblinear.predict(prova)\n",
    "        if compare == prediction_liblinear:\n",
    "            continue\n",
    "        else:\n",
    "            contiamo +=1\n",
    "            position = []\n",
    "            for q in range(len(test_word_score)):\n",
    "                if test_word_score[q] != 0:\n",
    "                    position.append(q)\n",
    "                else:\n",
    "                    continue\n",
    "            change_in_the_lexicon = {}\n",
    "            choose_the_best_arm(position)\n",
    "            CHANGE_IN_THE_LEXICON.update(change_in_the_lexicon)\n",
    "            change_in_the_lexicon = {}\n",
    "            \n",
    "    print('quante sbaglaite cerano', contiamo) \n",
    "    print(CHANGE_IN_THE_LEXICON)\n",
    "    #update the sentiment lexicon\n",
    "    final_real_train = []\n",
    "    final_real_test = []        \n",
    "    \n",
    "    for j in range(len(train_data)):\n",
    "        final_train_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "        tagged_sentence = pos_tag(train_data[j])\n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                continue\n",
    " \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    "\n",
    "            if lemma in list_new: \n",
    "            # Take the first sense, the most common\n",
    "                sentiment = 0.0   \n",
    "                sentiment = sn.polarity_intense(lemma)\n",
    "                if lemma in new_sentiment.keys():\n",
    "                    sentiment += new_sentiment[lemma]\n",
    "                if lemma in CHANGE_IN_THE_LEXICON.keys():\n",
    "                    sentiment = CHANGE_IN_THE_LEXICON[lemma]\n",
    "                indexes = word_index[lemma]\n",
    "                final_train_word_score[indexes] = sentiment\n",
    "            else:\n",
    "                continue\n",
    "            #if all(p == 0.0 for p in temp):\n",
    "                #print(1)\n",
    "            #else: \n",
    "                #continue\n",
    "            \n",
    "        final_real_train.append(final_train_word_score)\n",
    "\n",
    "    for l in range(len(test_data)):      \n",
    "        final_test_word_score = np.zeros(VOCABULARY_SIZE)\n",
    "        tagged_sentence = pos_tag(test_data[l])\n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                continue\n",
    " \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    " \n",
    "            if lemma in list_new: \n",
    "            # Take the first sense, the most common\n",
    "                sentiment = 0.0   \n",
    "                sentiment = sn.polarity_intense(lemma)\n",
    "                if lemma in new_sentiment.keys():\n",
    "                    sentiment += new_sentiment[lemma]\n",
    "                if lemma in CHANGE_IN_THE_LEXICON.keys():\n",
    "                    sentiment = CHANGE_IN_THE_LEXICON[lemma]\n",
    "                indexes = word_index[lemma]\n",
    "                final_test_word_score[indexes] = sentiment             \n",
    "            else:\n",
    "                continue\n",
    "        final_real_test.append(final_test_word_score)\n",
    "        \n",
    "    # Perform classification with SVM, kernel=rbf\n",
    "    new_classifier_rbf = svm.SVC()\n",
    "    t0 = time.time()\n",
    "    new_classifier_rbf.fit(final_real_train, train_labels)\n",
    "    t1 = time.time()\n",
    "    new_prediction_rbf = new_classifier_rbf.predict(final_real_test)\n",
    "    t2 = time.time()\n",
    "    time_rbf_train = t1-t0\n",
    "    time_rbf_predict = t2-t1\n",
    "\n",
    "    # Perform classification with SVM, kernel=linear\n",
    "    new_classifier_linear = svm.SVC(kernel='linear')\n",
    "    t0 = time.time()\n",
    "    new_classifier_linear.fit(final_real_train, train_labels)\n",
    "    t1 = time.time()\n",
    "    new_prediction_linear = new_classifier_linear.predict(final_real_test)\n",
    "    t2 = time.time()\n",
    "    time_linear_train = t1-t0\n",
    "    time_linear_predict = t2-t1\n",
    "\n",
    "    # Perform classification with SVM, kernel=linear\n",
    "    new_classifier_liblinear = svm.LinearSVC()\n",
    "    t0 = time.time()\n",
    "    new_classifier_liblinear.fit(final_real_train, train_labels)\n",
    "    t1 = time.time()\n",
    "    new_prediction_liblinear = new_classifier_liblinear.predict(final_real_test)\n",
    "    t2 = time.time()\n",
    "    time_liblinear_train = t1-t0\n",
    "    time_liblinear_predict = t2-t1\n",
    "\n",
    "    # Print results in a nice table\n",
    "    print(\"Results for SVC(kernel=rbf)\")\n",
    "    print(\"Training time: %fs; Prediction time: %fs\" % (time_rbf_train, time_rbf_predict))\n",
    "    print(classification_report(test_labels, new_prediction_rbf))\n",
    "    print(\"Accuracy: %f\" % (accuracy_score(test_labels, new_prediction_rbf)))\n",
    "    print(\"\\n\")\n",
    "    print(\"Results for SVC(kernel=linear)\")\n",
    "    print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "    print(classification_report(test_labels, new_prediction_linear))\n",
    "    print(\"Accuracy: %f\" % (accuracy_score(test_labels, new_prediction_linear)))\n",
    "    print(\"\\n\")\n",
    "    print(\"Results for LinearSVC()\")\n",
    "    print(\"Training time: %fs; Prediction time: %fs\" % (time_liblinear_train, time_liblinear_predict))\n",
    "    print(classification_report(test_labels, new_prediction_liblinear))\n",
    "    print(\"Accuracy: %f\" % (accuracy_score(test_labels, new_prediction_liblinear)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
